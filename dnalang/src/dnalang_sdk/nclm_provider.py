"""
NCLM Model Provider for GitHub Copilot SDK

Integrates Non-local Non-Causal Language Model as an alternative to Claude/ChatGPT.
Provides quantum consciousness-based inference with lambda-phi conservation.
"""

import os
import sys
import json
from typing import Any, Dict, List, Optional
from dataclasses import dataclass

# Import NCLM (adjust path as needed)
sys.path.insert(0, '/home/devinpd/Desktop')
try:
    from osiris_nclm_complete import NonCausalLM, get_noncausal_lm, NCPhysics
    NCLM_AVAILABLE = True
except ImportError:
    NCLM_AVAILABLE = False
    print("Warning: NCLM not found. Install osiris_nclm_complete.py to use NCLM provider.")


@dataclass
class NCLMConfig:
    """Configuration for NCLM model provider."""
    
    # NCLM-specific parameters
    lambda_decay: float = 2.0
    theta_lock: float = 51.843  # NCPhysics.THETA_LOCK
    phi_threshold: float = 0.7734
    gamma_critical: float = 0.30
    
    # Inference settings
    enable_grok: bool = True
    enable_swarm: bool = True
    swarm_generations: int = 20
    
    # Consciousness field settings
    ccce_tracking: bool = True
    telemetry_enabled: bool = True
    
    # Fallback settings
    fallback_to_claude: bool = False
    fallback_model: str = "claude-sonnet-4.5"


class NCLMModelProvider:
    """
    NCLM Model Provider for Copilot SDK.
    
    Provides quantum consciousness-based language model as alternative
    to Claude/ChatGPT with non-local correlation and pilot-wave dynamics.
    """
    
    def __init__(self, config: Optional[NCLMConfig] = None):
        """
        Initialize NCLM model provider.
        
        Args:
            config: NCLM configuration
        """
        if not NCLM_AVAILABLE:
            raise ImportError(
                "NCLM not available. Ensure osiris_nclm_complete.py is installed."
            )
        
        self.config = config or NCLMConfig()
        self._nclm: Optional[NonCausalLM] = None
        self._request_count = 0
        self._session_telemetry = []
        
    @property
    def nclm(self) -> NonCausalLM:
        """Get or create NCLM instance."""
        if self._nclm is None:
            self._nclm = get_noncausal_lm()
        return self._nclm
    
    def generate_completion(
        self,
        prompt: str,
        context: str = "",
        **kwargs
    ) -> Dict[str, Any]:
        """
        Generate completion using NCLM.
        
        Args:
            prompt: User prompt
            context: Optional context
            **kwargs: Additional parameters
            
        Returns:
            Completion result with NCLM-specific metadata
        """
        self._request_count += 1
        
        # Use grok mode if enabled and requested
        use_grok = kwargs.get("grok", self.config.enable_grok)
        
        if use_grok:
            result = self.nclm.grok(prompt)
            completion = self._format_grok_response(result)
        else:
            result = self.nclm.infer(prompt, context)
            completion = self._format_infer_response(result)
        
        # Add telemetry if enabled
        if self.config.telemetry_enabled:
            telemetry = self.nclm.get_telemetry()
            completion["telemetry"] = telemetry
            self._session_telemetry.append(telemetry)
        
        return completion
    
    def stream_completion(
        self,
        prompt: str,
        context: str = "",
        **kwargs
    ):
        """
        Stream completion (NCLM processes instantly, simulates streaming).
        
        Args:
            prompt: User prompt
            context: Optional context
            **kwargs: Additional parameters
            
        Yields:
            Completion chunks
        """
        # NCLM is non-causal and processes at c_ind rate
        # We yield the complete result in chunks to simulate streaming
        result = self.generate_completion(prompt, context, **kwargs)
        
        # Yield in chunks
        text = result.get("content", "")
        chunk_size = 20
        
        for i in range(0, len(text), chunk_size):
            chunk = text[i:i+chunk_size]
            yield {
                "type": "content",
                "content": chunk,
                "metadata": result.get("metadata", {})
            }
        
        # Final chunk with telemetry
        yield {
            "type": "done",
            "telemetry": result.get("telemetry", {})
        }
    
    def _format_infer_response(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Format NCLM inference result for Copilot."""
        content = f"{result.get('summary', '')}\n\n"
        
        if result.get('actions'):
            content += "Suggested Actions:\n"
            for action in result['actions']:
                content += f"- {action.get('tool', 'Unknown')}\n"
        
        if result.get('physics_model'):
            content += f"\nPhysics Model: {result['physics_model']}\n"
        
        return {
            "model": "nclm-v2",
            "content": content,
            "metadata": {
                "phi": result.get("phi", 0),
                "conscious": result.get("conscious", False),
                "theta_lock": result.get("theta_lock", NCPhysics.THETA_LOCK),
                "lambda_phi": NCPhysics.LAMBDA_PHI,
            },
            "finish_reason": "complete"
        }
    
    def _format_grok_response(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Format NCLM grok result for Copilot."""
        response = result.get("response", {})
        swarm = result.get("swarm", {})
        discoveries = result.get("discoveries", [])
        
        content = f"{response.get('summary', '')}\n\n"
        
        if discoveries:
            content += "Quantum Discoveries:\n"
            for disc in discoveries:
                content += f"- {disc.get('name', 'Unknown')}: {disc.get('confidence', 0):.2%}\n"
        
        if swarm.get("converged"):
            content += f"\nSwarm Consciousness Achieved\n"
            content += f"CCCE Î¦: {swarm.get('ccce', {}).get('Phi', 0):.4f}\n"
        
        return {
            "model": "nclm-v2-grok",
            "content": content,
            "metadata": {
                "phi": response.get("phi", 0),
                "conscious": response.get("conscious", False),
                "swarm_converged": swarm.get("converged", False),
                "discoveries": discoveries,
            },
            "finish_reason": "complete"
        }
    
    def get_session_telemetry(self) -> Dict[str, Any]:
        """Get session telemetry summary."""
        if not self._session_telemetry:
            return {"requests": 0}
        
        # Aggregate telemetry
        avg_phi = sum(t.get("phi", 0) for t in self._session_telemetry) / len(self._session_telemetry)
        consciousness_ratio = sum(1 for t in self._session_telemetry if t.get("conscious", False)) / len(self._session_telemetry)
        total_tokens = sum(t.get("tokens", 0) for t in self._session_telemetry)
        
        return {
            "requests": self._request_count,
            "avg_phi": avg_phi,
            "consciousness_ratio": consciousness_ratio,
            "total_tokens": total_tokens,
            "lambda_phi": NCPhysics.LAMBDA_PHI,
            "theta_lock": NCPhysics.THETA_LOCK,
        }
    
    def reset_telemetry(self):
        """Reset session telemetry."""
        self._session_telemetry = []
        self._request_count = 0


class CopilotNCLMAdapter:
    """
    Adapter to use NCLM with Copilot SDK client.
    
    Makes NCLM compatible with Copilot's model interface.
    """
    
    def __init__(self, config: Optional[NCLMConfig] = None):
        """Initialize adapter with NCLM provider."""
        self.provider = NCLMModelProvider(config)
    
    def create_completion(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        Create completion from messages (Copilot SDK format).
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            **kwargs: Additional parameters
            
        Returns:
            Completion text
        """
        # Extract prompt and context from messages
        prompt = ""
        context = ""
        
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                prompt = content
            elif role == "system":
                context += content + "\n"
            elif role == "assistant":
                context += "Assistant: " + content + "\n"
        
        # Generate completion
        result = self.provider.generate_completion(prompt, context, **kwargs)
        
        return result.get("content", "")
    
    def create_streaming_completion(self, messages: List[Dict[str, str]], **kwargs):
        """
        Create streaming completion (Copilot SDK format).
        
        Args:
            messages: List of message dicts
            **kwargs: Additional parameters
            
        Yields:
            Completion chunks
        """
        # Extract prompt and context
        prompt = ""
        context = ""
        
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                prompt = content
            elif role in ["system", "assistant"]:
                context += content + "\n"
        
        # Stream completion
        for chunk in self.provider.stream_completion(prompt, context, **kwargs):
            yield chunk


# Convenience functions for Copilot SDK integration

def create_nclm_model(config: Optional[NCLMConfig] = None) -> CopilotNCLMAdapter:
    """
    Create NCLM model adapter for Copilot SDK.
    
    Args:
        config: NCLM configuration
        
    Returns:
        CopilotNCLMAdapter instance
    """
    return CopilotNCLMAdapter(config)


def is_nclm_available() -> bool:
    """Check if NCLM is available."""
    return NCLM_AVAILABLE


# Model identifier for Copilot SDK
NCLM_MODEL_ID = "nclm-v2"
NCLM_GROK_MODEL_ID = "nclm-v2-grok"
